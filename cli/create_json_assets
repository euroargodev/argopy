#!/usr/bin/env python
# coding: utf-8

# # Create json data files for the static/assets folder

# In[1]:


import json
import pandas as pd
import numpy as np

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

import argopy
from argopy import ArgoNVSReferenceTables
from argopy.stores import filestore


output_folder = "../argopy/static/assets/"


# # data_types.json
# 
# For all possible Argo variables, give the expected data type
# 

# ## str

# In[2]:


list_str = [
    "PLATFORM_NUMBER",
    "DATA_MODE",
    "DIRECTION",
    "DATA_CENTRE",
    "DATA_TYPE",
    "FORMAT_VERSION",
    "HANDBOOK_VERSION",
    "PROJECT_NAME",
    "PI_NAME",
    "STATION_PARAMETERS",
    "DATA_CENTER",
    "DC_REFERENCE",
    "DATA_STATE_INDICATOR",
    "PLATFORM_TYPE",
    "FIRMWARE_VERSION",
    "POSITIONING_SYSTEM",
    "PARAMETER",
    "SCIENTIFIC_CALIB_EQUATION",
    "SCIENTIFIC_CALIB_COEFFICIENT",
    "SCIENTIFIC_CALIB_COMMENT",
    "HISTORY_INSTITUTION",
    "HISTORY_STEP",
    "HISTORY_SOFTWARE",
    "HISTORY_SOFTWARE_RELEASE",
    "HISTORY_REFERENCE",
    "HISTORY_QCTEST",
    "HISTORY_ACTION",
    "HISTORY_PARAMETER",
    "VERTICAL_SAMPLING_SCHEME",
    "FLOAT_SERIAL_NO",
    "PARAMETER_DATA_MODE",

    # Trajectory file variables:
    'TRAJECTORY_PARAMETERS', 'POSITION_ACCURACY', 'GROUNDED', 'SATELLITE_NAME', 'HISTORY_INDEX_DIMENSION',

    # Technical file variables:
    'TECHNICAL_PARAMETER_NAME', 'TECHNICAL_PARAMETER_VALUE', 'PTT',

    # Metadata file variables:
    'END_MISSION_STATUS',
    'TRANS_SYSTEM',
    'TRANS_SYSTEM_ID',
    'TRANS_FREQUENCY',
    'PLATFORM_FAMILY',
    'PLATFORM_MAKER',
    'MANUAL_VERSION',
    'STANDARD_FORMAT_ID',
    'DAC_FORMAT_ID',
    'ANOMALY',
    'BATTERY_TYPE',
    'BATTERY_PACKS',
    'CONTROLLER_BOARD_TYPE_PRIMARY',
    'CONTROLLER_BOARD_TYPE_SECONDARY',
    'CONTROLLER_BOARD_SERIAL_NO_PRIMARY',
    'CONTROLLER_BOARD_SERIAL_NO_SECONDARY',
    'SPECIAL_FEATURES',
    'FLOAT_OWNER',
    'OPERATING_INSTITUTION',
    'CUSTOMISATION',
    'DEPLOYMENT_PLATFORM',
    'DEPLOYMENT_CRUISE_ID',
    'DEPLOYMENT_REFERENCE_STATION_ID',
    'LAUNCH_CONFIG_PARAMETER_NAME',
    'CONFIG_PARAMETER_NAME',
    'CONFIG_MISSION_COMMENT',
    'SENSOR',
    'SENSOR_MAKER',
    'SENSOR_MODEL',
    'SENSOR_SERIAL_NO',
    'PARAMETER_SENSOR',
    'PARAMETER_UNITS',
    'PARAMETER_ACCURACY',
    'PARAMETER_RESOLUTION',
    'PREDEPLOYMENT_CALIB_EQUATION',
    'PREDEPLOYMENT_CALIB_COEFFICIENT',
    'PREDEPLOYMENT_CALIB_COMMENT',
]
[list_str.append("PROFILE_{}_QC".format(v)) for v in list(ArgoNVSReferenceTables().tbl(3)["altLabel"])];
[list_str.append("{}_DATA_MODE".format(v)) for v in list(ArgoNVSReferenceTables().tbl(3)["altLabel"])];


# ## int

# In[3]:


list_int = [
    "PLATFORM_NUMBER",
    "WMO_INST_TYPE",
    "CYCLE_NUMBER",
    "CONFIG_MISSION_NUMBER",

    # Trajectory file variables:
    'JULD_STATUS', 'JULD_ADJUSTED_STATUS', 'JULD_DESCENT_START_STATUS',
    'JULD_FIRST_STABILIZATION_STATUS', 'JULD_DESCENT_END_STATUS', 'JULD_PARK_START_STATUS', 'JULD_PARK_END_STATUS',
    'JULD_DEEP_DESCENT_END_STATUS', 'JULD_DEEP_PARK_START_STATUS', 'JULD_DEEP_ASCENT_START_STATUS',
    'JULD_ASCENT_START_STATUS', 'JULD_ASCENT_END_STATUS', 'JULD_TRANSMISSION_START_STATUS',
    'JULD_FIRST_MESSAGE_STATUS', 'JULD_FIRST_LOCATION_STATUS', 'JULD_LAST_LOCATION_STATUS',
    'JULD_LAST_MESSAGE_STATUS', 'JULD_TRANSMISSION_END_STATUS', 'REPRESENTATIVE_PARK_PRESSURE_STATUS',
]


# ## datetime

# In[4]:


list_datetime = [
    "REFERENCE_DATE_TIME",
    "DATE_CREATION",
    "DATE_UPDATE",
    "JULD",
    "JULD_LOCATION",
    "SCIENTIFIC_CALIB_DATE",
    "HISTORY_DATE",
    "TIME",

    # Metadata file variables:
    'LAUNCH_DATE', 'START_DATE', 'STARTUP_DATE', 'END_MISSION_DATE',
]


# ## save data

# In[5]:


data = {
  "name": "data_types",
  "long_name": "Expected data types of Argo variables",
  "last_update": pd.to_datetime('now', utc=True).isoformat(),
  "data": {
      'str': list_str,
      'int': list_int,
      'datetime': list_datetime,
  }
}
# print(json.dumps(data))
with open(output_folder + "data_types.json", 'w') as f:
    json.dump(data, f, indent=2)


# In[6]:


# Test load:
result = filestore().open_json(output_folder + "data_types.json")
result.keys(), result['data'].keys()


# # dict_institutions

# In[7]:


nvs = ArgoNVSReferenceTables(cache=True)
institutions = {}
for row in nvs.tbl(4).iterrows():
    institutions.update({row[1]['altLabel']: row[1]['prefLabel']})
institutions    


# ## Save data

# In[8]:


data = {
  "name": "institutions",
  "long_name": "Institution names from Argo reference table 4",
  "last_update": pd.to_datetime('now', utc=True).isoformat(),
  "data": {
      'institutions': institutions,
  }
}
# print(json.dumps(data))
with open(output_folder + "institutions.json", 'w') as f:
    json.dump(data, f, indent=2)


# In[9]:


# Test load:
result = filestore().open_json(output_folder + "institutions.json")
result.keys(), result['data'].keys()


# # dict profilers

# In[10]:


nvs = ArgoNVSReferenceTables(cache=True)
profilers = {}
for row in nvs.tbl(8).iterrows():
    profilers.update({row[1]['altLabel']: row[1]['prefLabel']})
profilers    


# ## Save data

# In[11]:


data = {
  "name": "profilers",
  "long_name": "Profiler codes and description from Argo reference table 8",
  "last_update": pd.to_datetime('now', utc=True).isoformat(),
  "data": {
      'profilers': profilers,
  }
}
# print(json.dumps(data))
with open(output_folder + "profilers.json", 'w') as f:
    json.dump(data, f, indent=2)


# In[12]:


# Test load:
result = filestore().open_json(output_folder + "profilers.json")
result.keys(), result['data'].keys()


# # API-Coriolis: parameter codes
# 
# Content to work with https://api-coriolis.ifremer.fr/legacy/parameter?code=%s
# 
# Make dict to work with parameter codes

# In[13]:


store = argopy.stores.httpstore(cache=False)
store


# In[14]:


def get_codemeta(x):
    try:
        data = store.open_json("https://api-coriolis.ifremer.fr/legacy/parameter?code=%s" % str(x))
        return data
    except:
        return None
    
valid_codes = []    
for code in np.arange(1, 1000):
    code_meta = get_codemeta(code)
    if code_meta:
        # print(code, code_meta['label'])
        valid_codes.append(code)
print("Found %i valid codes" % len(valid_codes))   
print(valid_codes)


# In[15]:


def get_code_for_param(name):
    strict = PARAMS.loc[PARAMS['gf3_strict_name'] == name]
    extended = PARAMS.loc[PARAMS['gf3_extended_name'] == name]
    if strict.shape[0]>=1:
        use = strict
    else:
        use = extended
    return int(use['code'].values[0])

def get_param_for_code(code):
    row = PARAMS.loc[PARAMS['code']==str(code)]
    if row['gf3_strict_name'].values[0]:
        return row['gf3_strict_name'].values[0]
    else:
        return row['gf3_extended_name'].values[0]


# In[16]:


# Retrieve data:    
# parameter_codes = [28,35,30, 66,68,70, 67,69,71]  # core-Argo variables P,T,S (PARAM, PARAM_ADJUSTED, PARAM_ERROR)
parameter_codes = valid_codes
PARAMS = {'unit': [], 
          'time_sampling': [], 
          'code': [], 
          'gf3_strict_name': [], 
          'gf3_extended_name': [],
          'parent_code': [],
          'label': [],
          'param_type': [],          
         }
for code in parameter_codes:
    code_meta = get_codemeta(code)
    for key, value in code_meta.items():
        PARAMS[key].append(value)
PARAMS = pd.DataFrame(PARAMS)        


# In[17]:


# Make useful dict to go from code to param and from param to code:
# (we also update the list of valid_codes to retain only those with a non empty name)
code_to_param = {}
param_to_code = {}
record_valid_codes = []
for ii, param in PARAMS.iterrows():
    param_name = get_param_for_code(param['code'])
    param_code = get_code_for_param(param_name)
    if param_name != '':
        record_valid_codes.append(param_code)
        code_to_param.update({str(param_code): param.to_dict()})
        param_to_code.update({param_name: param.to_dict()})
        # print(ii, param_code, param_name)
        
print("We'll record %i parameter data" % len(record_valid_codes))        


# ## Save data

# In[18]:


data = {
  "name": "parameters",
  "long_name": "All valid requests to https://api-coriolis.ifremer.fr/legacy/parameter?code={code}",
  "last_update": pd.to_datetime('now', utc=True).isoformat(),
  "data": {
      "valid_codes": [str(v) for v in record_valid_codes],  # We use str because it's key to use in code_to_param dict
      "valid_params": list(param_to_code.keys()),
      "codes": code_to_param,
      "params": param_to_code,
  }
}

with open(output_folder + "api_coriolis_parameter_codes.json", 'w') as f:
    json.dump(data, f, indent=2, default=str)


# In[19]:


# Test load:
result = filestore().open_json(output_folder + "api_coriolis_parameter_codes.json")
result.keys(), result['data'].keys()


# # Catalogue for ArgoDocs

# In[20]:


catalogue = [
    {
        "category": "Argo data formats",
        "title": "Argo user's manual",
        "doi": "10.13155/29825",
        "id": 29825
    },
    {
        "category": "Quality control",
        "title": "Argo Quality Control Manual for CTD and Trajectory Data",
        "doi": "10.13155/33951",
        "id": 33951
    },
    {
        "category": "Quality control",
        "title": "Argo quality control manual for dissolved oxygen concentration",
        "doi": "10.13155/46542",
        "id": 46542
    },
    {
        "category": "Quality control",
        "title": "Argo quality control manual for biogeochemical data",
        "doi": "10.13155/40879",
        "id": 40879
    },
    {
        "category": "Quality control",
        "title": "BGC-Argo quality control manual for the Chlorophyll-A concentration",
        "doi": "10.13155/35385",
        "id": 35385
    },
    {
        "category": "Quality control",
        "title": "BGC-Argo quality control manual for nitrate concentration",
        "doi": "10.13155/84370",
        "id": 84370
    },
    {
        "category": "Quality control",
        "title": "Quality control for BGC-Argo radiometry",
        "doi": "10.13155/62466",
        "id": 62466
    },
    {
        "category": "Cookbooks",
        "title": "Argo DAC profile cookbook",
        "doi": "10.13155/41151",
        "id": 41151
    },
    {
        "category": "Cookbooks",
        "title": "Argo DAC trajectory cookbook",
        "doi": "10.13155/29824",
        "id": 29824
    },
    {
        "category": "Cookbooks",
        "title": "DMQC Cookbook for Core Argo parameters",
        "doi": "10.13155/78994",
        "id": 78994
    },
    {
        "category": "Cookbooks",
        "title": "Processing Argo oxygen data at the DAC level",
        "doi": "10.13155/39795",
        "id": 39795
    },
    {
        "category": "Cookbooks",
        "title": "Processing Bio-Argo particle backscattering at the DAC level",
        "doi": "10.13155/39459",
        "id": 39459
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo chlorophyll-A concentration at the DAC level",
        "doi": "10.13155/39468",
        "id": 39468
    },
    {
        "category": "Cookbooks",
        "title": "Processing Argo measurement timing information at the DAC level",
        "doi": "10.13155/47998",
        "id": 47998
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo CDOM concentration at the DAC level",
        "doi": "10.13155/54541",
        "id": 54541
    },
    {
        "category": "Cookbooks",
        "title": "Processing Bio-Argo nitrate concentration at the DAC Level",
        "doi": "10.13155/46121",
        "id": 46121
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo Radiometric data at the DAC level",
        "doi": "10.13155/51541",
        "id": 51541
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo pH data at the DAC level",
        "doi": "10.13155/57195",
        "id": 57195
    },
    {
        "category": "Cookbooks",
        "title": "Description of the Argo GDAC File Checks: Data Format and Consistency Checks",
        "doi": "10.13155/46120",
        "id": 46120
    },
    {
        "category": "Cookbooks",
        "title": "Description of the Argo GDAC File Merge Process",
        "doi": "10.13155/52154",
        "id": 52154
    },
    {
        "category": "Cookbooks",
        "title": "BGC-Argo synthetic profile file processing and format on Coriolis GDAC",
        "doi": "10.13155/55637",
        "id": 55637
    },
    {
        "category": "Cookbooks",
        "title": "Argo GDAC cookbook",
        "doi": "10.13155/46202",
        "id": 46202
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo pH data at the DAC level",
        "doi": "10.13155/57195",
        "id": 57195
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo nitrate concentration at the DAC Level",
        "doi": "10.13155/46121",
        "id": 46121
    },
    {
        "category": "Cookbooks",
        "title": "Processing BGC-Argo pH data at the DAC level",
        "doi": "10.13155/57195",
        "id": 57195
    },
    {
        "category": "Quality Control",
        "title": "BGC-Argo quality control manual for pH",
        "doi": "10.13155/97828",
        "id": 97828
    },
]


# ## Save data

# In[21]:


data = {
  "name": "ADMT documentation catalogue",
  "long_name": "Titles and DOIs of all the official ADMT documentation",
  "last_update": pd.to_datetime('now', utc=True).isoformat(),
  "data": {
      'catalogue': catalogue,
  }
}
# print(json.dumps(data))
with open(output_folder + "admt_documentation_catalogue.json", 'w') as f:
    json.dump(data, f, indent=2)


# In[22]:


# Test load:
result = filestore().open_json(output_folder + "admt_documentation_catalogue.json")
result.keys(), result['data'].keys()


# # List of BGC variables

# ## Synthetic file variables

# In[23]:


# # We get the list of variables from the ArgoIndex
# from argopy import ArgoIndex
# params = ArgoIndex(index_file='bgc-s').load().read_params()


# In[24]:


# We get the list of variables from the Ifremer Erddap, serving the S files:
from argopy.stores import httpstore
data = httpstore().open_json(
                "https://erddap.ifremer.fr/erddap" + "/info/ArgoFloats-synthetic-BGC/index.json"
            )
bgc_vlist_erddap = [row[1].upper() for row in data["table"]["rows"] if row[0] == "variable"]
bgc_vlist_erddap.sort()


# ### Save data

# In[25]:


data = {
  "name": "BGC synthetic netcdf files variables",
  "long_name": "Variables from the Ifremer Erddap ArgoFloats-synthetic-BGC dataset based on GDAC synthetic netcdf files",
  "last_update": pd.to_datetime('now', utc=True).isoformat(),
  "data": {
      'variables': bgc_vlist_erddap,
  }
}
# print(json.dumps(data))
with open(output_folder + "variables_bgc_synthetic.json", 'w') as f:
    json.dump(data, f, indent=2)
